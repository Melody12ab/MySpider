#data into file system location root
root_dir=D:/MbmServer_data
#httpclient  parameters
HttpGet.http.socket.timeout=5000
#最大链接数
max_connections=80
#获取链接的最大等待时间
wait_connection_timeout=15000
#链接超时时间
connection_timeout=15000
#读数据超时时间
read_timeout=50000

#spider time control
#api或是cookie方式抓取每次的间隔时间
api_spider_interval_time=10000000000
#cookie_spider_interval_time=1000
cookie_spider_interval_time=2000

#daemon
#守护线程监视系统每次的间隔时间--周期时间
daemon_monitor_interval_time=10000
daemon_write_success_rest_time=3000

#spider grab control
#api抓取时每页要请求的微言条数
page_items=200

#控制微博平台的抓取系统是否开启
sina_spider_enable=true
#sina_spider_enable=false
#qq_spider_enable=true
qq_spider_enable=false

#控制两个抓取器的开关
api_spider_enable=false
#api_spider_enable=true
cookie_spider_enable=true
#cookie_spider_enable=false
#keyword_spider_enable=false
keyword_spider_enable=true
keyword_user_switch_enable=false
#topic_title_spider_enable=false
#topic_title_spider_enable=true
topic_title_spider_enable=false
#topic_content_spider_enable=false
#topic_content_spider_enable=true
topic_content_spider_enable=false
#是否单独更换topic相应的cookie,或是根据登陆用户的改变而改变
topic_title_single_user_switch_enable=false
topic_content_single_user_switch_enable=false
#设置代理功能是否启用
proxy_open=false
#proxy_open=true
#设置启动哪个功能代理，如关键词和非关键词之分
keyword_proxy_open=false

#关于爬虫扩散控制的设置
forward_account_uid_add_tovisit=false
doc_url_add_comment_tovisit=false
comment_uid_add_account_tovisit=true
comment_uid_add_doc_tovisit=true

#输入errorUrls的文件存放路径
error_urls_file_path=D:/MbmServer_data/errorUrls.txt
#bloomFilter写进文件的位置
doc_bloomFilter_file_path=doc_bloomFilter.dat
visitedUrls_bloomFilter_file_path=visitedUrls_bloomFilter.dat
errorUrls_bloomFilter_file_path=errorUrls_bloomFilter.dat
sina_personInfo_bloomFilter_file_path=sina_personInfo_bloomFilter.dat
check_repeat_sina_personInfo_bloomFilter_file_path=check_repeat_sina_personInfo_bloomFilter.dat
toVisitedUrls_list_file_path=toVisitedUrls_List.dat
visitedAttentionUrls_set_file_path=visitedAttentionUrls_set_List.dat

#mongoDB的bloomFilter设置--开始
commentDoc_bloomFilter_file_path=commentDoc_bloomFilter.dat
commentDoc_filter_size=32
toVisitCommentUrls_file_path=toVisitCommentUrls_file_path.dat
#mongoDB的bloomFilter设置--结束

doc_filter_size=32
visitedUrls_filter_size=32
errorUrls_filter_size=16
sina_personInfo_filter_size=32
check_repeat_sina_personInfo_filter_size=32

#守护线程运行多少个周期之后，将现有的bloomFilter写入到文件中
bloomFilter_outputFile_circle=600

#一个url当抓取出现错误时，最多重复抓取它几次
url_repeat_count=1
keyword_repeat_count=10
#最大的同时登陆的帐户数量,因为现在搜索的抓取和其它抓取分开，帮设为2，不要改变
max_api_user_online_number=1
#抓取微博关注的最大关注页数,20个/页
grab_attention_max_pageNum=10
api_grab_attention_max_sum_count=3000
#抓取attention时，每个抓取的休息时间
grab_attention_sleep_interval_time=1500
#api请求的baseUrl
api_base_url=https://api.weibo.com/2/
#SwitchUserDaemonThread多长时间切换一次用户
#switch_user_interval_time=1000000
#sina_switch_account_interval_time=1340
#sina_switch_account_interval_time=50
sina_switch_account_interval_time=30
#sina_switch_account_interval_time=60
#sina_switch_account_interval_time=1
#sina_switch_account_interval_time=1340
qq_switch_account_interval_time=1340
switch_search_interval_time=1340

switch_user_thread_sleep_time=5000

switch_user_once_fail_sleep_time=600000

#关于keyword的设置
visitedKeyWords_bloomFilter_file_path=visitedKeywords_bloomFilter.dat
toVisitKeyWords_file_path=toVisitKeyWords_List.dat
visitedKeyWords_filter_size=32

keyWordDoc_bloomFilter_file_path=KeywordsDoc_bloomFilter.dat
keyWordsDoc_filter_size=32

#keyword抓取每页后修息的时间长度
keyword_grab_one_page_intervale=6000
keyword_or_cookie_null_task_waiting=3000
#特征词提取时的参数配置
feature_doc_number=100
hotwords_doc_number=500
feature_word_number=20
dic_file_path=library/library.dic library/new_library.dic
new_dic_file_path=D:/MbmServer_data/new_library.dic
addNewWords_filter_size=16
featureWords_cache_file_path=featureWords_cache.dat

#关于url to uid的对应关系配置
UUMap_cache_file_path=UUMap.dat

#关于url to uid的对应关系配置
UPMap_cache_file_path=UPMap.dat

#关于周期任务参数配置
#monitor_list_circle_time以分钟为单位
#monitor_list_circle_time=10
monitor_list_circle_time=60
#url_task_circle_time以分钟为单位
url_task_circle_time=120

keyword_task_circle_time=60

#url_task_circle_time以分钟为单位,来做对于媒体版微博抓取的时间设置
media_url_task_circle_time=2
enterprise_url_task_circle_time=3
profession_url_task_circle_time=3

circle_task_later=60
circle_urls_cache_file_path=circle_urls_cache.dat
circle_visitedKeyWords_cache_file_path=circle_visitedKeyWords_cache.dat

#关于状态查询时，出现临界查询时的重复查询次数
status_max_select_number=5

#关于ieVersion的设定
IE_Version=6.0 7.0 8.0 9.0

#关于ip proxy集合的配置文件路径
ip_proxy_file_path=proxy

#每个内容页，包括搜索显示页内容，其内容重复条数若大于当下的指定的最大重数条数的时候即认为抓取应该结束
max_content_page_repeat_items=20
max_content_page_repeat_items_comments=20

#LjSearch ip与端口设置  for 中国网
search_accountInfo_ip=127.0.0.1
search_accountInfo_port=8000
search_attentionInfo_ip=127.0.0.1
search_attentionInfo_port=8001
search_contentInfo_ip=127.0.0.1
search_contentInfo_port=8002

#LjSearch ip与端口设置 for 缔元信
search_doc_ip=127.0.0.1
search_doc_port=8000

#关于备份搜索ip与端口的配置---开始
search_accountInfo_ip_bak=124.238.192.53
search_accountInfo_port_bak=8000
search_attentionInfo_ip_bak=124.238.192.53
search_attentionInfo_port_bak=8001
search_contentInfo_ip_bak=124.238.192.53
search_contentInfo_port_bak=8002
#LjSearch ip与端口设置 for 缔元信
search_doc_ip_bak=124.238.192.53
search_doc_port_bak=8000
#关于备份搜索ip与端口的配置---结束

top_doc_number=100
#升级设置
#is_update=false

#当没有起止或是截止时间时，时间的活动区间,单位为分钟
before_after_interval_time=30
before_after_interval_time_4_lingjoin=1440

#关于LJsearch Socket数量的设置
person_search_socket_num=50
attention_search_socket_num=10
doc_search_socket_num=80

#关于socket请求紧张时候，重复请求得到空闲的socket的最大限制次数
request_socket_fail_times=5

#关于redis的server host ip 和 port的配置
redis_host=127.0.0.1
redis_port=6379
redis_password=sinx/cosx=tanx

#字符集设置
charset=UTF-8

#数据挖掘方面参数---------------開始
#hotwords的时间设置，从此刻起的之前多长时间，时间为分钟
hotwords_interval=144000
hot_topic_number=20
#话题方向参数设置
#以分钟为单位
Topic_Interval=1
hotTopic_cache_file_path=hotTopic_cache.dat
#开启抓取到的话题标题直接加入到其对应的话题内容列表当中
#topic_title_add_grab_task_enable=true
topic_title_add_grab_task_enable=false
#以秒为单位
topic_content_grab_too_fast_fresh_waitting_time=10

toVisitTopicTitle_file_path=toVisitTopicTitle_file_path.dat
circle_visitedTopicTitle_cache_file_path=circle_visitedTopicTitle_cache_file_path.dat
#数据挖掘方面参数---------------結束

#NLPIR配置
#nlpir data文件所在路径
NLPIR_DATA_PATH=./nlpir
NLPIR_DLL_PATH=nlpir/NLPIR_JNI
#对于NLPIR处理数据源的时候，0代表的gb2312,1代表的UTF-8,有下边的type是否传参需要，下边两个参数要对应
NLPIR_Process_Encoding=UTF-8
NLPIR_Process_Encoding_Type=1
NLPIR_Extract_Keyword_Number=20
#NLPIR配置--结束

#验证码最多可以输错几次
verify_max_error_time=10
#单位为秒
verify_print_interval_time=2
#验证码配置结束

#JZSearch Or command Number 
max_or_command_keyword_number=3

#log config
#log_output_file_enable=true
log_output_file_enable=false

#抓取时的策略参数设置
is_add_keyword_crawl_result_todo_task=false
